# train.py: Script for training a model using the dataset generated by the previous script.
# This script takes arguments to specify the dataset and other configurations.
#
# example launch command:
#     torchrun --nproc_per_node=NUM_GPUS axbench/scripts/train.py --config axbench/demo/sweep/train.yaml
import os
import argparse
import yaml
import json
import glob
import pickle
import torch
import shutil
import requests
import pandas as pd
import numpy as np
from transformers import AutoModelForCausalLM, AutoTokenizer
from huggingface_hub import hf_hub_download
from pathlib import Path
from args.training_args import TrainingArgs
from axbench.utils.constants import * 
from axbench.utils.model_utils import get_prefix_length, get_suffix_length
from transformers import set_seed
import torch.distributed as dist
import sys
from torch.utils.data import DataLoader

# all supported methods
import axbench

import logging

# Initialize the logger
logger = logging.getLogger(__name__)

CONFIG_FILE = "config.json"
STATE_FILE = "train_state.pkl"


def data_generator(data_dir, max_concepts=None):
    """
    Generator function to read data files and yield data subsets by concept_id.

    Args:
        data_dir (str): Path to the data directory.

    Yields:
        (concept_id, df_subset): A tuple containing the concept_id and subset DataFrame.
    """
    df = pd.read_parquet(os.path.join(data_dir, 'train_data.parquet'))
    concept_ids = df['concept_id'].unique()
    concept_ids.sort()
    
    if max_concepts is not None and max_concepts < len(concept_ids) and max_concepts > 0:
        concept_ids = concept_ids[:max_concepts]
        
    for concept_id in concept_ids:
        if concept_id >= 0:
            df_subset = df[df['concept_id'] == concept_id]
            yield (concept_id, df_subset)


def load_metadata(metadata_path):
    """
    Load metadata from a JSON lines file.
    """
    metadata = []
    with open(metadata_path, 'r') as f:
        for line in f:
            data = json.loads(line)
            metadata += [data]  # Return the metadata as is
    return metadata


def load_metadata_flatten(metadata_path):
    """
    Load flatten metadata from a JSON lines file.
    """
    metadata = []
    concept_id = 0
    with open(metadata_path, 'r') as f:
        for line in f:
            data = json.loads(line)
            concept, ref =data["concept"], data["ref"]
            concept_genres_map = data["concept_genres_map"][concept]
            ref = data["ref"]
            flatten_data = {
                "concept": concept,
                "ref": ref,
                "concept_genres_map": {concept: concept_genres_map},
                "concept_id": concept_id
            }
            metadata += [flatten_data]  # Return the metadata as is
            concept_id += 1
    return metadata


def prepare_df(original_df, concept, all_df, tokenizer, binarize, is_chat_model):
    suffix_length = get_suffix_length(tokenizer)
    if binarize:
        # assign input and output containing concept with 1, otherwise 0
        positive_df = original_df[original_df["output_concept"] == concept]
        negative_df = all_df[all_df["output_concept"] == EMPTY_CONCEPT]
        if is_chat_model:
            def apply_chat_template(row):
                messages = [
                    {"role": "user", "content": row["input"]},
                    {"role": "assistant", "content": row["output"]}
                ]
                nobos = tokenizer.apply_chat_template(messages, tokenize=True)[1:-suffix_length]
                return tokenizer.decode(nobos)
            positive_df = positive_df.copy()
            negative_df = negative_df.copy()
            positive_df['combined'] = positive_df.apply(apply_chat_template, axis=1)
            negative_df['combined'] = negative_df.apply(apply_chat_template, axis=1)
        else:
            positive_df = positive_df.copy()
            negative_df = negative_df.copy()
            positive_df['combined'] = positive_df['input'] + positive_df['output']
            negative_df['combined'] = negative_df['input'] + negative_df['output']
        positive_df = pd.DataFrame(positive_df[['combined']]).rename(columns={'combined': 'input'})
        negative_df = pd.DataFrame(negative_df[['combined']]).rename(columns={'combined': 'input'})
        positive_df["labels"] = 1
        negative_df["labels"] = 0
        return pd.concat([positive_df, negative_df], axis=0)
    else:
        # if not binarizing, we need to apply the chat template to the input. It becomes a standard instruction tuning task.
        if is_chat_model:
            def apply_chat_template(row):
                messages = [{"role": "user", "content": row["input"]}]
                nobos = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True)[1:]
                return tokenizer.decode(nobos)
            original_df['input'] = original_df.apply(apply_chat_template, axis=1)
        return original_df # do nothing, the task will be standard instruction tuning.

def partition_list(lst, n):
    """
    Partition a list into n approximately equal slices.

    Args:
        lst (list): The list to partition.
        n (int): The number of partitions.

    Returns:
        list of lists: A list containing n sublists.
    """
    k, m = divmod(len(lst), n)
    return [lst[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n)]


def load_state(dump_dir, rank):
    """
    Load the state from a file if it exists.
    """
    state_path = os.path.join(f"{dump_dir}", f"{STATE_FILE}_rank_{rank}")
    if os.path.exists(state_path):
        with open(state_path, "rb") as f:
            return pickle.load(f)
    return None


def save_state(dump_dir, state, rank):
    dump_dir = Path(dump_dir)
    dump_dir.mkdir(parents=True, exist_ok=True)
    # Save state
    state_path = os.path.join(dump_dir, f"{STATE_FILE}_rank_{rank}")
    with open(state_path, "wb") as f:
        pickle.dump(state, f)


def main():
   

    args = TrainingArgs(section="train")

    # Initialize the process group
    dist.init_process_group(backend='nccl', init_method='env://')

    # Get the rank and world_size from environment variables
    rank = dist.get_rank()
    world_size = dist.get_world_size()
    local_rank = int(os.environ.get('LOCAL_RANK', 0))

    # Set the device for this process
    device = torch.device(f'cuda:{local_rank}')
    torch.cuda.set_device(device)

    # Set a unique seed per rank for reproducibility
    set_seed(args.seed + rank)

    if args.overwrite_data_dir and Path(args.overwrite_data_dir).exists():
        logger.warning(f"Overwriting data directory {args.data_dir}")
        args.data_dir = args.overwrite_data_dir
    else:
        args.data_dir = f"{args.dump_dir}/generate"

    # Configure the logger per rank
    logger.setLevel(logging.WARNING)  # Set the logging level as desired

    # Create a logging formatter that includes the rank
    formatter = logging.Formatter(
        fmt=f'%(asctime)s,%(msecs)03d %(levelname)-8s [Rank {rank}] [%(filename)s:%(lineno)d] %(message)s',
        datefmt='%Y-%m-%d:%H:%M:%S'
    )

    # Create a console handler and set its formatter
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(formatter)

    # Add the handler to the logger
    if not logger.handlers:
        logger.addHandler(console_handler)

    # Optionally, create a file handler per rank
    """
    log_file = f'log_rank_{rank}.log'
    file_handler = logging.FileHandler(log_file)
    file_handler.setFormatter(formatter)
    logger.addHandler(file_handler)
    """

    # Load dataset and metadata
    metadata_path = os.path.join(args.data_dir, 'metadata.jsonl')
    metadata = load_metadata(metadata_path)
    df_generator = data_generator(args.data_dir, max_concepts=args.max_concepts)
    all_df = pd.read_parquet(os.path.join(args.data_dir, 'train_data.parquet')) # this is needed for binarizing the dataset
    df_list = list(df_generator)

    dump_dir = Path(args.dump_dir) / "train"
    dump_dir.mkdir(parents=True, exist_ok=True)

    # Load tokenizer
    tokenizer = AutoTokenizer.from_pretrained(args.model_name, model_max_length=512)
    tokenizer.padding_side = "right"

    # Partition df_list among ranks
    df_list_per_rank = partition_list(df_list, world_size)
    my_df_list = df_list_per_rank[rank]

    # Load model instance onto device
    if args.use_bf16:
        logger.warning(f"Using bfloat16 for model {args.model_name}")
    model_instance = AutoModelForCausalLM.from_pretrained(
        args.model_name, torch_dtype=torch.bfloat16 if args.use_bf16 else None)
    is_chat_model = True if args.model_name in CHAT_MODELS else False
    model_instance = model_instance.eval()
    model_instance.to(device)

    prefix_length = 1 # prefix is default to 1 for all models due to theBOS token.
    if is_chat_model:
        prefix_length = get_prefix_length(tokenizer)
        logger.warning(f"Chat model prefix length: {prefix_length}")

    state = load_state(dump_dir, rank)
    last_concept_id = state.get("last_concept_id", None) if state else None
    logger.warning(f"Rank {rank} last concept_id processed: {last_concept_id}")

    # Run training for assigned concept_ids
    for concept_id, concept_df in my_df_list:
        if last_concept_id is not None and concept_id <= last_concept_id:
            logger.warning(f"Rank {rank} skipping concept_id {concept_id} because it is already processed")
            continue
        logger.warning(f"Training models for concept_id {concept_id} on rank {rank}")
        for model_name in sorted(args.models.keys()):
            concept = metadata[concept_id]["concept"]
            logger.warning(f"Training {model_name} with concept {concept}")
            benchmark_model = getattr(axbench, model_name)(
                model_instance, tokenizer, layer=args.layer,
                training_args=args.models[model_name],
                device=device, seed=args.seed, 
            )
            low_rank_dimension = args.models[model_name].low_rank_dimension \
                if args.models[model_name].low_rank_dimension else 1
            benchmark_model.make_model(
                mode="train",
                low_rank_dimension=low_rank_dimension,
                dtype=torch.bfloat16 if args.use_bf16 else None,
                intervention_type=args.models[model_name].intervention_type,
                concept_id=concept_id
            )
            if model_name not in {"LoReFT", "LoRA", "SFT"} and args.use_bf16:
                benchmark_model.ax.to(torch.bfloat16)
            kwargs = {
                "prefix_length": prefix_length,
                "positions": args.models[model_name].intervention_positions,
                "exclude_bos": args.models[model_name].exclude_bos,
            }
            prepared_df = concept_df.copy()
            prepared_df = prepare_df(
                prepared_df, concept, all_df, tokenizer, 
                binarize=args.models[model_name].binarize_dataset, is_chat_model=is_chat_model
            )
            benchmark_model.train(prepared_df, **kwargs)
            benchmark_model.save(dump_dir, model_name=f"rank_{rank}_{model_name}")
            if model_name == "SFT":
                # we need to reload the original model after SFT.
                if args.use_bf16:
                    logger.warning(f"Using bfloat16 for model {args.model_name}")
                model_instance = AutoModelForCausalLM.from_pretrained(
                    args.model_name, torch_dtype=torch.bfloat16 if args.use_bf16 else None)
                is_chat_model = True if args.model_name in CHAT_MODELS else False
                model_instance = model_instance.eval()
                model_instance.to(device)
            logger.warning(f"Saved weights and biases for model {model_name} on rank {rank}")
            # Clean up
            del benchmark_model
            torch.cuda.empty_cache()
        # After processing, save state
        current_state = {'last_concept_id': concept_id}
        save_state(dump_dir, current_state, rank)

    # Synchronize all processes
    dist.barrier()

    # Rank 0 merges results
    if rank == 0:
        logger.warning("Rank 0 is merging results.")

        # Save other config
        config = {"model_name": args.model_name,
                "layer": args.layer,
                "component": args.component}
        config_path = dump_dir / CONFIG_FILE
        with open(config_path, 'w') as f:
            json.dump(config, f)

        for model_name in sorted(args.models.keys()):
            # Collect per-rank weight and bias files
            weight_files = [dump_dir / f"rank_{r}_{model_name}_weight.pt" for r in range(world_size)]
            bias_files = [dump_dir / f"rank_{r}_{model_name}_bias.pt" for r in range(world_size)]

            # Check if files exist
            weight_files_existing = [f for f in weight_files if f.exists()]
            bias_files_existing = [f for f in bias_files if f.exists()]

            if not weight_files_existing or not bias_files_existing:
                logger.warning(f"No weight or bias files found for model {model_name}. Skipping.")
                continue

            # Load weights and biases
            weights = [torch.load(f) for f in weight_files_existing]
            biases = [torch.load(f) for f in bias_files_existing]

            # Concatenate weights and biases
            if isinstance(weights[0], dict):
                merged_weight = {}
                for key in weights[0].keys():
                    weight_tensors = [w[key] for w in weights]
                    merged_weight[key] = torch.cat(weight_tensors, dim=0)
            else:
                merged_weight = torch.cat(weights, dim=0)

            # Handle dictionary biases
            if isinstance(biases[0], dict):
                merged_bias = {}
                for key in biases[0].keys():
                    bias_tensors = [b[key] for b in biases]
                    merged_bias[key] = torch.cat(bias_tensors, dim=0)
            else:
                merged_bias = torch.cat(biases, dim=0)

            # Save merged weight and bias files
            weight_file = dump_dir / f"{model_name}_weight.pt"
            bias_file = dump_dir / f"{model_name}_bias.pt"
            torch.save(merged_weight, weight_file)
            torch.save(merged_bias, bias_file)
            logger.warning(f"Saved merged weights and biases for model {model_name}")

            # Optionally delete per-rank files
            for f in weight_files_existing + bias_files_existing:
                try:
                    f.unlink()
                    logger.warning(f"Deleted file {f.name}")
                except Exception as e:
                    logger.error(f"Error deleting file {f.name}: {e}")

        # Save SAE weights and biases for inference
        logger.warning("Saving SAE weights and biases for inference")
        flatten_metadata = load_metadata_flatten(metadata_path)
        # Save pruned SAE weights and biases
        sae_path = flatten_metadata[0]["ref"].split("https://www.neuronpedia.org/")[-1]
        sae_url = f"https://www.neuronpedia.org/api/feature/{sae_path}"
        headers = {"X-Api-Key": os.environ.get("NP_API_KEY")}
        response = requests.get(sae_url, headers=headers).json()
        hf_repo = response["source"]["hfRepoId"]
        hf_folder = response["source"]["hfFolderId"]
        path_to_params = hf_hub_download(
            repo_id=hf_repo,
            filename=f"{hf_folder}/params.npz",
            force_download=False,
        )
        params = np.load(path_to_params)
        sae_pt_params = {k: torch.from_numpy(v) for k, v in params.items()}
        pruned_sae_pt_params = {
            "b_dec": sae_pt_params["b_dec"],
            "W_dec": [],
            "W_enc": [],
            "b_enc": [],
            "threshold": []
        }
        for concept_id, metadata in enumerate(flatten_metadata):
            sae_id = int(metadata["ref"].split("/")[-1])
            pruned_sae_pt_params["W_dec"].append(sae_pt_params["W_dec"][[sae_id], :])
            pruned_sae_pt_params["W_enc"].append(sae_pt_params["W_enc"][:, [sae_id]])
            pruned_sae_pt_params["b_enc"].append(sae_pt_params["b_enc"][[sae_id]])
            pruned_sae_pt_params["threshold"].append(sae_pt_params["threshold"][[sae_id]])
        for k, v in pruned_sae_pt_params.items():
            if k == "b_dec":
                continue
            if k == "W_enc":
                pruned_sae_pt_params[k] = torch.cat(v, dim=1)
            else:
                pruned_sae_pt_params[k] = torch.cat(v, dim=0)
        torch.save(pruned_sae_pt_params, dump_dir / "GemmaScopeSAE.pt") # sae only has one file

    # Finalize the process group
    dist.destroy_process_group()

    # Remove handlers to prevent duplication if the script is run multiple times
    logger.removeHandler(console_handler)
    # If file_handler is used, remove it as well
    # logger.removeHandler(file_handler)


if __name__ == "__main__":
    main()

