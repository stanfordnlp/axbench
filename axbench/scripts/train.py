# train.py: Script for training a model using the dataset generated by the previous script.
# This script takes arguments to specify the dataset and other configurations.
#
# example launch command:
#     torchrun --nproc_per_node=NUM_GPUS axbench/scripts/train.py --config axbench/demo/sweep/train.yaml
import os
import argparse
import yaml
import json
import glob
import pickle
import torch
import shutil
import requests
import pandas as pd
import numpy as np
from transformers import AutoModelForCausalLM, AutoTokenizer
from huggingface_hub import hf_hub_download
from pathlib import Path
from args.training_args import TrainingArgs
from axbench.utils.constants import * 
from axbench.utils.model_utils import get_prefix_length, get_suffix_length
from transformers import set_seed
import torch.distributed as dist
import sys
from torch.utils.data import DataLoader
from axbench.models.sae import save_pruned_sae

# all supported methods
import axbench

import logging

# Initialize the logger
logger = logging.getLogger(__name__)

CONFIG_FILE = "config.json"
STATE_FILE = "train_state.pkl"
METADATA_FILE = "metadata.jsonl"


def data_generator(data_dir):
    """
    Generator function to read multiple data files and yield data subsets by concept_id.
    Processes files in order: train_data.parquet, train_data_0.parquet, train_data_1.parquet, etc.

    Args:
        data_dir (str): Path to the data directory.

    Yields:
        (concept_id, df_subset): A tuple containing the concept_id and subset DataFrame.
    """
    # Gather all file paths in the directory
    file_paths = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.startswith('train_data') and f.endswith('.parquet')]

    # Sort files: 'train_data.parquet' comes first, then 'train_data_X.parquet' sorted by X
    def extract_index(file_name):
        if file_name == 'train_data.parquet':
            return -1  # Ensure 'train_data.parquet' comes first
        else:
            # Extract the number X from 'train_data_X.parquet'
            return int(file_name.split('_')[-1].split('.')[0])

    file_paths.sort(key=lambda x: extract_index(os.path.basename(x)))

    for file_path in file_paths:
        df = pd.read_parquet(file_path)
        concept_ids = df['concept_id'].unique()
        concept_ids.sort()
        for concept_id in concept_ids:
            if concept_id >= 0:
                print(f"Processing concept_id {concept_id}")
                df_subset = df[df['concept_id'] == concept_id]
                yield (concept_id, df_subset)


def load_metadata(metadata_path):
    """
    Load metadata from a JSON lines file.
    """
    metadata = []
    with open(metadata_path, 'r') as f:
        for line in f:
            data = json.loads(line)
            metadata += [data]  # Return the metadata as is
    return metadata


def prepare_df(original_df, negative_df, concept, metadata, tokenizer, binarize, train_on_negative, is_chat_model, max_num_of_examples=None, suppress_steer_dict_path = None):
    
    suffix_length = get_suffix_length(tokenizer)
    genre = metadata["concept_genres_map"][concept][0]
    # assign input and output containing concept with 1, otherwise 0
    positive_df = original_df[(original_df["output_concept"] == concept) & (original_df["category"] == "positive")]
    negative_df = negative_df[(negative_df["concept_genre"] == genre)]
    if suppress_steer_dict_path is not None:
        with open(suppress_steer_dict_path, 'r') as f:
            suppress_steer_dict = json.load(f)
            s = True
    else:
        s = False
    if max_num_of_examples:
        positive_df = positive_df.head(max_num_of_examples // 2)
        negative_df = negative_df.head(max_num_of_examples // 2)
    if binarize:
        if is_chat_model:
            def apply_chat_template(row):
                if s:
                    messages = [
                        {"role": "user", "content": suppress_steer_dict[concept] + "\n\nQuestion:"+[row["input"]]},
                        {"role": "assistant", "content": row["output"]}
                    ]
                else:
                    messages = [
                        {"role": "user", "content": row["input"]},
                        {"role": "assistant", "content": row["output"]}
                    ]
                nobos = tokenizer.apply_chat_template(messages, tokenize=True)[1:-suffix_length]
                return tokenizer.decode(nobos)
            positive_df = positive_df.copy()
            negative_df = negative_df.copy()
            positive_df['combined'] = positive_df.apply(apply_chat_template, axis=1)
            negative_df['combined'] = negative_df.apply(apply_chat_template, axis=1)
        else:
            positive_df = positive_df.copy()
            negative_df = negative_df.copy()
            positive_df['combined'] = positive_df['input'] + positive_df['output']
            negative_df['combined'] = negative_df['input'] + negative_df['output']
        positive_df = pd.DataFrame(positive_df[['combined']]).rename(columns={'combined': 'input'})
        negative_df = pd.DataFrame(negative_df[['combined']]).rename(columns={'combined': 'input'})

        positive_df["labels"] = 1
        negative_df["labels"] = 0

        return pd.concat([positive_df, negative_df], axis=0)
    else:        
        # if not binarizing, we need to apply the chat template to the input. It becomes a standard instruction tuning task.
        if train_on_negative:
            all_df = pd.concat([positive_df, negative_df], axis=0)
        else:          
            if s:
                all_df = negative_df
            else:
                all_df = positive_df
        
        if is_chat_model:
            def apply_chat_template(row):
                if s:
                    messages = [{"role": "user", "content": suppress_steer_dict[concept] + "\n\nQuestion:"+row["input"]}]
                else:
                    messages = [{"role": "user", "content": row["input"]}]       
                nobos = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True)[1:]
                return tokenizer.decode(nobos)
            all_df['input'] = all_df.apply(apply_chat_template, axis=1)
        return all_df # do nothing, the task will be standard instruction tuning.

def partition_list(lst, n):
    """
    Partition a list into n approximately equal slices.

    Args:
        lst (list): The list to partition.
        n (int): The number of partitions.

    Returns:
        list of lists: A list containing n sublists.
    """
    k, m = divmod(len(lst), n)
    return [lst[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n)]


def load_state(dump_dir, rank):
    """
    Load the state from a file if it exists.
    """
    state_path = os.path.join(f"{dump_dir}", f"{STATE_FILE}_rank_{rank}")
    if os.path.exists(state_path):
        with open(state_path, "rb") as f:
            return pickle.load(f)
    return None


def save_state(dump_dir, state, concept_metadata, rank):
    dump_dir = Path(dump_dir)
    dump_dir.mkdir(parents=True, exist_ok=True)
    # Save state
    state_path = os.path.join(dump_dir, f"{STATE_FILE}_rank_{rank}")
    with open(state_path, "wb") as f:
        pickle.dump(state, f)

    # Save metadata again
    metadata_path = os.path.join(dump_dir, f"rank_{rank}_{METADATA_FILE}")
    with open(metadata_path, "a") as f:
        f.write(json.dumps(concept_metadata) + "\n")

def main():
   
    args = TrainingArgs(section="train")

    # Initialize the process group
    dist.init_process_group(backend='nccl', init_method='env://')

    # Get the rank and world_size from environment variables
    rank = dist.get_rank()
    world_size = dist.get_world_size()
  
    local_rank = int(os.environ.get('LOCAL_RANK', 0))

    # Set the device for this process
    device = torch.device(f'cuda:{local_rank}')
    torch.cuda.set_device(device)

    # Set a unique seed per rank for reproducibility
    set_seed(args.seed + rank)

    if args.overwrite_data_dir and Path(args.overwrite_data_dir).exists():
        logger.warning(f"Overwriting data directory {args.data_dir}")
        args.data_dir = args.overwrite_data_dir
    else:
        args.data_dir = f"{args.dump_dir}/generate"

    # Configure the logger per rank
    logger.setLevel(logging.WARNING)  # Set the logging level as desired

    # Create a logging formatter that includes the rank
    formatter = logging.Formatter(
        fmt=f'%(asctime)s,%(msecs)03d %(levelname)-8s [Rank {rank}] [%(filename)s:%(lineno)d] %(message)s',
        datefmt='%Y-%m-%d:%H:%M:%S'
    )

    # Create a console handler and set its formatter
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(formatter)

    # Add the handler to the logger
    if not logger.handlers:
        logger.addHandler(console_handler)

    # Optionally, create a file handler per rank
    """
    log_file = f'log_rank_{rank}.log'
    file_handler = logging.FileHandler(log_file)
    file_handler.setFormatter(formatter)
    logger.addHandler(file_handler)
    """

    # Load dataset and metadata
    metadata_path = os.path.join(args.data_dir, 'metadata.jsonl')
    metadata = load_metadata(metadata_path)
    df_generator = data_generator(args.data_dir)

    all_df = pd.read_parquet(os.path.join(args.data_dir, 'train_data.parquet')) # this is needed for binarizing the dataset
    negative_df = all_df[(all_df["output_concept"] == EMPTY_CONCEPT) & (all_df["category"] == "negative")]
    df_list = list(df_generator)
    
    logger.warning(f"Total number of concept df loaded: {len(df_list)}")
    if args.max_concepts:
        logger.warning(f"All ranks only processing {args.max_concepts} concepts")
        df_list = df_list[:args.max_concepts]

    dump_dir = Path(args.dump_dir) / "train"
    dump_dir.mkdir(parents=True, exist_ok=True)
    
    # save pruned SAE
    sae_params = None # TODO: this is a workaround to avoid breaking the code.
    if rank == 0:
        sae_params = save_pruned_sae(metadata_path, dump_dir)

    # Load tokenizer
    tokenizer = AutoTokenizer.from_pretrained(args.model_name, model_max_length=512)
    tokenizer.padding_side = "right"
    tokenizer.add_special_tokens({'pad_token': '<|finetune_right_pad_id|>'})

    # Partition df_list among ranks
    df_list_per_rank = partition_list(df_list, world_size)
    my_df_list = df_list_per_rank[rank]

    # Load model instance onto device
    if args.use_bf16:
        logger.warning(f"Using bfloat16 for model {args.model_name}")
    model_instance = AutoModelForCausalLM.from_pretrained(
        args.model_name, torch_dtype=torch.bfloat16 if args.use_bf16 else None)
    is_chat_model = True if args.model_name in CHAT_MODELS else False
    model_instance = model_instance.eval()
    model_instance.to(device)

    prefix_length = 1 # prefix is default to 1 for all models due to theBOS token.
    if is_chat_model:
        prefix_length = get_prefix_length(tokenizer)
        logger.warning(f"Chat model prefix length: {prefix_length}")

    state = load_state(dump_dir, rank)
    last_concept_id = state.get("last_concept_id", None) if state else None
    logger.warning(f"Rank {rank} last concept_id processed: {last_concept_id}")

    # Run training for assigned concept_ids
    # logger.warning(metadata)

    for concept_id, concept_df in my_df_list:
        concept_id = int(concept_id)
        if last_concept_id is not None and concept_id <= last_concept_id:
            logger.warning(f"Rank {rank} skipping concept_id {concept_id} because it is already processed")
            continue
        logger.warning(f"Training models for concept_id {concept_id} on rank {rank}")
        for model_name in sorted(args.models.keys()):
            concept = metadata[concept_id]["concept"]
            logger.warning(f"Training {model_name} with concept {concept}")
            benchmark_model = getattr(axbench, model_name)(
                model_instance, tokenizer, layer=args.layer,
                training_args=args.models[model_name],
                lm_model_name=args.model_name,
                device=device, seed=args.seed, 
            )
            low_rank_dimension = args.models[model_name].low_rank_dimension \
                if args.models[model_name].low_rank_dimension else 1
            benchmark_model.make_model(
                mode="train",
                low_rank_dimension=low_rank_dimension,
                dtype=torch.bfloat16 if args.use_bf16 else None,
                intervention_type=args.models[model_name].intervention_type,
                concept_id=concept_id,
                sae_params=sae_params,
                metadata_path=metadata_path,
                dump_dir=dump_dir,
            )
            if model_name not in {"LoReFT", "LoRA", "SFT"} and args.use_bf16:
                benchmark_model.ax.to(torch.bfloat16)
            kwargs = {
                "prefix_length": prefix_length,
                "positions": args.models[model_name].intervention_positions,
                "exclude_bos": args.models[model_name].exclude_bos,
                "metadata_path": metadata_path,
                "output_length": 128,
            }
            prepared_df = concept_df.copy()
            prepared_df = prepare_df(
                prepared_df, negative_df, concept, metadata[concept_id], tokenizer, 
                binarize=args.models[model_name].binarize_dataset, 
                train_on_negative=args.models[model_name].train_on_negative,
                is_chat_model=is_chat_model,
                max_num_of_examples=args.max_num_of_examples,
                suppress_steer_dict_path=args.steer_suppress_dict_path
            )
            if args.models[model_name].intervention_type == "anneal":
                benchmark_model.train_anneal(prepared_df, **kwargs)
            elif args.models[model_name].intervention_type == "noise" or args.models[model_name].intervention_type == "factor":
                benchmark_model.train_noise(prepared_df, **kwargs)
            else:
                benchmark_model.train(prepared_df, **kwargs)
            benchmark_model.save(dump_dir, model_name=f"rank_{rank}_{model_name}")
            
            if args.models[model_name].intervention_type == "factor":
                print("training factor")
                benchmark_model.train_factor(prepared_df, concept_id, dump_dir, model_name=f"rank_{rank}_{model_name}",**kwargs)

            if args.models[model_name].intervention_type == "gating" or args.models[model_name].intervention_type == "anneal":
                benchmark_model.save_gating(dump_dir, model_name=f"rank_{rank}_{model_name}")
            
            if args.models[model_name].intervention_type == "factor":
                benchmark_model.save_factor_gating(dump_dir, model_name=f"rank_{rank}_{model_name}")

            if model_name == "SFT":
                # we need to reload the original model after SFT.
                if args.use_bf16:
                    logger.warning(f"Using bfloat16 for model {args.model_name}")
                model_instance = AutoModelForCausalLM.from_pretrained(
                    args.model_name, torch_dtype=torch.bfloat16 if args.use_bf16 else None)
                is_chat_model = True if args.model_name in CHAT_MODELS else False
                model_instance = model_instance.eval()
                model_instance.to(device)
            if model_name == "LoRA":
                model_instance = benchmark_model.ax_model.unload()
            logger.warning(f"Saved weights and biases for model {model_name} on rank {rank}")
            # Clean up
            del benchmark_model
            torch.cuda.empty_cache()
        # After processing, save state
        current_state = {'last_concept_id': concept_id}
        save_state(dump_dir, current_state, metadata[concept_id], rank)

    # Synchronize all processes
    dist.barrier()

    # Rank 0 merges results
    if rank == 0:
        logger.warning("Rank 0 is merging results.")
        # Merging metadata
        metadata_entries = []
        for r in range(world_size):
            metadata_path = os.path.join(dump_dir, f"rank_{r}_{METADATA_FILE}")
            with open(metadata_path, "r") as f:
                for line in f:
                    metadata_entry = json.loads(line)
                    metadata_entries.append(metadata_entry)
        metadata_path = os.path.join(dump_dir, METADATA_FILE)
        with open(metadata_path, "a") as f:
            for metadata_entry in metadata_entries:
                f.write(json.dumps(metadata_entry) + "\n")

        # Save other config
        config = {"model_name": args.model_name,
                "layer": args.layer,
                "component": args.component}
        config_path = dump_dir / CONFIG_FILE
        with open(config_path, 'w') as f:
            json.dump(config, f)

        for model_name in sorted(args.models.keys()):
            # Collect per-rank weight and bias files
            weight_files = [dump_dir / f"rank_{r}_{model_name}_weight.pt" for r in range(world_size)]
            bias_files = [dump_dir / f"rank_{r}_{model_name}_bias.pt" for r in range(world_size)]
            weight_files_existing = [f for f in weight_files if f.exists()]
            bias_files_existing = [f for f in bias_files if f.exists()]

            if args.models[model_name].intervention_type == "gating" or args.models[model_name].intervention_type == "factor" or args.models[model_name].intervention_type == "anneal":
                weight_files_gating = [dump_dir / f"rank_{r}_{model_name}_gating_weight.pt" for r in range(world_size)]
                bias_files_gating = [dump_dir / f"rank_{r}_{model_name}_gating_bias.pt" for r in range(world_size)]
                weight_files_gating_existing = [f for f in weight_files_gating if f.exists()]
                bias_files_gating_existing = [f for f in bias_files_gating if f.exists()]

            if not weight_files_existing or not bias_files_existing:
                logger.warning(f"No weight or bias files found for model {model_name}. Skipping.")
                continue
            
            def write_files(weight_files_existing, bias_files_existing, weight_file, bias_file):
                # Check if files exist
                # Load weights and biases
                weights = [torch.load(f) for f in weight_files_existing]
                biases = [torch.load(f) for f in bias_files_existing]

                # Concatenate weights and biases
                if isinstance(weights[0], dict):
                    merged_weight = {}
                    for key in weights[0].keys():
                        weight_tensors = [w[key] for w in weights]
                        merged_weight[key] = torch.cat(weight_tensors, dim=0)
                else:
                    merged_weight = torch.cat(weights, dim=0)

                # Handle dictionary biases
                if isinstance(biases[0], dict):
                    merged_bias = {}
                    for key in biases[0].keys():
                        bias_tensors = [b[key] for b in biases]
                        merged_bias[key] = torch.cat(bias_tensors, dim=0)
                else:
                    merged_bias = torch.cat(biases, dim=0)

                # Save merged weight and bias files

                torch.save(merged_weight, weight_file)
                torch.save(merged_bias, bias_file)
                logger.warning(f"Saved merged weights and biases for model {model_name}")

                # Optionally delete per-rank files
                for f in weight_files_existing + bias_files_existing:
                    try:
                        f.unlink()
                        logger.warning(f"Deleted file {f.name}")
                    except Exception as e:
                        logger.error(f"Error deleting file {f.name}: {e}")
            
            write_files(weight_files_existing, bias_files_existing, dump_dir / f"{model_name}_weight.pt", dump_dir / f"{model_name}_bias.pt")

            if args.models[model_name].intervention_type == "gating" or args.models[model_name].intervention_type == "factor" or args.models[model_name].intervention_type == "anneal":
                write_files(weight_files_gating_existing, bias_files_gating_existing, dump_dir / f"{model_name}_gating_weight.pt", dump_dir / f"{model_name}_gating_bias.pt")
    
    # Finalize the process group
    dist.destroy_process_group()

    # Remove handlers to prevent duplication if the script is run multiple times
    logger.removeHandler(console_handler)
    # If file_handler is used, remove it as well
    # logger.removeHandler(file_handler)


if __name__ == "__main__":
    main()

