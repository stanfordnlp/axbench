# train.py: Script for training a model using the dataset generated by the previous script.
# This script takes arguments to specify the dataset and other configurations.
#
# example launch command:
#     python axbench/scripts/train.py --config axbench/demo/sweep/train.yaml

try:
    # This library is our indicator that the required installs
    # need to be done.
    import pyreax

except ModuleNotFoundError:
    # relative import; better to pip install subctrl
    import sys
    sys.path.append("../../pyreax")
    import pyreax

import os, argparse, yaml, json, glob, pickle, torch
import shutil
import pandas as pd
from transformers import AutoModelForCausalLM, AutoTokenizer
from pathlib import Path
from args.training_args import TrainingArgs
from concurrent.futures import ThreadPoolExecutor, as_completed
from transformers import set_seed
import queue

# all supported methods
import axbench


import logging
logging.basicConfig(format='%(asctime)s,%(msecs)03d %(levelname)-8s [%(filename)s:%(lineno)d] %(message)s',
    datefmt='%Y-%m-%d:%H:%M:%S',
    level=logging.WARN)
logger = logging.getLogger(__name__)

STATE_FILE = "train_state.pkl"
CONFIG_FILE = "config.json"


def data_generator(data_dir, seed):
    """
    Generator function to read data files and yield data subsets by group_id.

    Args:
        data_dir (str): Path to the data directory.

    Yields:
        (group_id, df_subset): A tuple containing the group_id and subset DataFrame.
    """
    # Get list of files sorted by index
    df = pd.read_parquet(os.path.join(data_dir, 'train_data.parquet'))
    group_ids = df['group_id'].unique()
    group_ids.sort()
    for group_id in group_ids:
        df_subset = df[df['group_id'] == group_id]
        df_subset = df_subset.sample(frac=1, random_state=seed)
        yield (group_id, df_subset)


def load_metadata(metadata_path):
    """
    Load metadata from a JSON lines file.
    """
    metadata = []
    with open(metadata_path, 'r') as f:
        for line in f:
            data = json.loads(line)
            metadata += [data]  # Return the metadata as is
    return metadata


def load_state(dump_dir):
    """
    Load the state from a file if it exists.
    
    Args:
        dump_dir (str): The directory to load the state file from.
    
    Returns:
        dict: The loaded state dictionary, or None if no state file exists.
    """
    state_path = os.path.join(f"{dump_dir}/train", STATE_FILE)
    if os.path.exists(state_path):
        with open(state_path, "rb") as f:
            return pickle.load(f)
    return None
    

def save(args, group_id, models):
    """save artifacts"""
    
    # handle training df first
    dump_dir = args.dump_dir
    dump_dir = Path(dump_dir) / "train"
    dump_dir.mkdir(parents=True, exist_ok=True)
    
    for model in models:
        model.save(dump_dir)

    state_path = dump_dir / STATE_FILE
    with open(state_path, "wb") as f:
        pickle.dump({"group_id": group_id + 1}, f)

    # save other config
    config = {"model_name": args.model_name,
        "layer": args.layer,
        "component": args.component}
    config_path = dump_dir / CONFIG_FILE
    with open(config_path, 'w') as f:
        json.dump(config, f)


def binarize_df(original_df, concept, model_name):
    if model_name in {
        "LinearProbe", "L1LinearProbe", "IntegratedGradients",
        "InputXGradients", "IntegratedGradients",
        "Random", "MeanEmbedding", "MeanActivation", "MeanPositiveActivation"
    }:
        # assign input and output containing concept with 1, otherwise 0
        input_df = original_df[original_df["input_concept"]==concept]
        output_df = original_df[original_df["output_concept"]==concept]
        positive_df = pd.concat([input_df["input"], output_df["output"]], axis=0).reset_index(drop=True)
        positive_df = pd.DataFrame(positive_df, columns=['input'])

        input_df = original_df[original_df["input_concept"]!=concept]
        output_df = original_df[original_df["output_concept"]!=concept]
        negative_df = pd.concat([input_df["input"], output_df["output"]], axis=0).reset_index(drop=True)
        negative_df = pd.DataFrame(negative_df, columns=['input'])

        positive_df["labels"] = 1
        negative_df["labels"] = 0

        return pd.concat([positive_df, negative_df], axis=0)
    else:
        # not implemented
        raise NotImplementedError(f"Binarization not implemented for {model_name}")


def main():
    args = TrainingArgs(section="train")
    set_seed(args.seed)
    args.data_dir = f"{args.dump_dir}/generate"
    # Load dataset and metadata
    metadata_path = os.path.join(args.data_dir, 'metadata.jsonl')
    metadata = load_metadata(metadata_path)
    df_generator = data_generator(args.data_dir, args.seed)
    df_list = list(df_generator)  # Collect all (group_id, group_df) pairs

    dump_dir = Path(args.dump_dir) / "train"
    dump_dir.mkdir(parents=True, exist_ok=True)

    # Load tokenizer
    tokenizer = AutoTokenizer.from_pretrained(args.model_name, model_max_length=512)
    tokenizer.padding_side = "right"

    state = load_state(args.dump_dir)
    start_group_id = state.get("group_id", 0) if state else 0
    logger.warning(f"Starting group index: {start_group_id}")

    # Prepare tasks
    tasks = []
    for model_name in args.models.keys():
        for group_id, group_df in df_list:
            if group_id < start_group_id:
                continue
            if model_name == "ReAX":
                tasks.append((model_name, group_id, group_df, None))
            else:
                for concept in metadata[group_id]["concepts"]:
                    tasks.append((model_name, group_id, group_df, concept))
    if len(tasks) == 0:
        logger.warning(f"No tasks to train. Exiting.")
        return

    # Initialize the available devices queue.
    available_devices = [f'cuda:{i}' for i in range(torch.cuda.device_count())]
    device_queue = queue.Queue()
    for device in available_devices:
        device_queue.put(device)

    device_models = {}
    for device in available_devices:
        model_instance = AutoModelForCausalLM.from_pretrained(args.model_name)
        model_instance.config.use_cache = False
        model_instance = model_instance.eval()
        model_instance = model_instance.to(device)
        device_models[device] = model_instance

    benchmark_model_results = {}

    # Process tasks in batches equal to the number of available GPUs
    torch.cuda.empty_cache()
    def run_train(task):
        model_name, group_id, group_df, concept = task
        # Get an available device from the queue
        device = device_queue.get()
        try:
            model_class = getattr(axbench, model_name)
            model_instance = device_models[device]
            if model_name == "ReAX":
                logger.warning(f"Training {model_class} with paired data: group_id {group_id} ({len(group_df)})\n")
                benchmark_model = model_class(
                    model_instance, tokenizer, layer=args.layer,
                    training_args=args.models[model_name],
                    device=device
                )
                benchmark_model.train(group_df)
            else:
                logger.warning(
                    f"Training {model_class} with non-paired data for concept {concept} ({len(group_df)})\n")
                benchmark_model = model_class(
                    model_instance, tokenizer, layer=args.layer,
                    training_args=args.models[model_name],
                    device=device
                )
                benchmark_model.train(binarize_df(group_df, concept, model_name))
            return model_name, group_id, concept, benchmark_model
        finally:
            device_queue.put(device)

    # Run tasks in parallel on available GPUs
    # Run the tasks in parallel using ThreadPoolExecutor
    with ThreadPoolExecutor() as executor:
        futures = {
            executor.submit(run_train, task): task
            for task in tasks
        }
        for future in as_completed(futures):
            task = futures[future]
            model_name, group_id, concept, benchmark_model = future.result()
            key = (model_name, group_id, concept)
            benchmark_model_results[key] = benchmark_model

    # Save all models after training
    for model_name in args.models.keys():
        for group_id, group_df in df_list:
            if group_id < start_group_id:
                continue
            if model_name == "ReAX":
                save(args, group_id, [benchmark_model_results[(model_name, group_id, None)]])
            else:
                for concept in metadata[group_id]["concepts"]:
                    save(args, group_id, [benchmark_model_results[(model_name, group_id, concept)]])


if __name__ == "__main__":
    main()

