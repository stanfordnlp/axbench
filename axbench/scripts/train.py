# train.py: Script for training a model using the dataset generated by the previous script.
# This script takes arguments to specify the dataset and other configurations.
#
# example launch command:
#     torchrun --nproc_per_node=NUM_GPUS axbench/scripts/train.py --config axbench/demo/sweep/train.yaml
import os
import argparse
import yaml
import json
import glob
import pickle
import torch
import shutil
import requests
import pandas as pd
import numpy as np
from transformers import AutoModelForCausalLM, AutoTokenizer
from huggingface_hub import hf_hub_download
from pathlib import Path
from args.training_args import TrainingArgs
from transformers import set_seed
import torch.distributed as dist
import sys
from torch.utils.data import DataLoader

# all supported methods
import axbench

import logging

# Initialize the logger
logger = logging.getLogger(__name__)

CONFIG_FILE = "config.json"
STATE_FILE = "train_state.pkl"


def data_generator(data_dir):
    """
    Generator function to read data files and yield data subsets by group_id.

    Args:
        data_dir (str): Path to the data directory.

    Yields:
        (group_id, df_subset): A tuple containing the group_id and subset DataFrame.
    """
    df = pd.read_parquet(os.path.join(data_dir, 'train_data.parquet'))
    group_ids = df['group_id'].unique()
    group_ids.sort()
    for group_id in group_ids:
        df_subset = df[df['group_id'] == group_id]
        yield (group_id, df_subset)


def load_metadata(metadata_path):
    """
    Load metadata from a JSON lines file.
    """
    metadata = []
    with open(metadata_path, 'r') as f:
        for line in f:
            data = json.loads(line)
            metadata += [data]  # Return the metadata as is
    return metadata


def load_metadata_flatten(metadata_path):
    """
    Load flatten metadata from a JSON lines file.
    """
    metadata = []
    group_id = 0
    with open(metadata_path, 'r') as f:
        for line in f:
            data = json.loads(line)
            for concept_id, concept in enumerate(data["concepts"]):
                concept_genres_map = data["concept_genres_map"][concept]
                contrast_concepts_map = data["contrast_concepts_map"][concept]
                ref = data["refs"][concept_id]
                flatten_data = {
                    "concept": concept,
                    "ref": ref,
                    "concept_genres_map": {concept: concept_genres_map},
                    "contrast_concepts_map": {concept: contrast_concepts_map},
                    "group_id": group_id
                }
                metadata += [flatten_data]  # Return the metadata as is
            group_id += 1
    return metadata


def binarize_df(original_df, concept, model_name):
    # assign input and output containing concept with 1, otherwise 0
    input_df = original_df[original_df["input_concept"] == concept]
    output_df = original_df[original_df["output_concept"] == concept]
    positive_df = pd.concat([input_df["input"], output_df["output"]], axis=0).reset_index(drop=True)
    positive_df = pd.DataFrame(positive_df, columns=['input'])

    input_df = original_df[original_df["input_concept"] != concept]
    output_df = original_df[original_df["output_concept"] != concept]
    negative_df = pd.concat([input_df["input"], output_df["output"]], axis=0).reset_index(drop=True)
    negative_df = pd.DataFrame(negative_df, columns=['input'])

    positive_df["labels"] = 1
    negative_df["labels"] = 0

    return pd.concat([positive_df, negative_df], axis=0)


def partition_list(lst, n):
    """
    Partition a list into n approximately equal slices.

    Args:
        lst (list): The list to partition.
        n (int): The number of partitions.

    Returns:
        list of lists: A list containing n sublists.
    """
    k, m = divmod(len(lst), n)
    return [lst[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n)]


def load_state(dump_dir, rank):
    """
    Load the state from a file if it exists.
    """
    state_path = os.path.join(f"{dump_dir}", f"{STATE_FILE}_rank_{rank}")
    if os.path.exists(state_path):
        with open(state_path, "rb") as f:
            return pickle.load(f)
    return None


def save_state(dump_dir, state, rank):
    dump_dir = Path(dump_dir)
    dump_dir.mkdir(parents=True, exist_ok=True)
    # Save state
    state_path = os.path.join(dump_dir, f"{STATE_FILE}_rank_{rank}")
    with open(state_path, "wb") as f:
        pickle.dump(state, f)


def main():
    args = TrainingArgs(section="train")

    # Initialize the process group
    dist.init_process_group(backend='nccl', init_method='env://')

    # Get the rank and world_size from environment variables
    rank = dist.get_rank()
    world_size = dist.get_world_size()
    local_rank = int(os.environ.get('LOCAL_RANK', 0))

    # Set the device for this process
    device = torch.device(f'cuda:{local_rank}')
    torch.cuda.set_device(device)

    # Configure PyTorch for determinism
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

    # Set a unique seed per rank for reproducibility
    set_seed(args.seed + rank)

    args.data_dir = f"{args.dump_dir}/generate"

    # Configure the logger per rank
    logger.setLevel(logging.WARNING)  # Set the logging level as desired

    # Create a logging formatter that includes the rank
    formatter = logging.Formatter(
        fmt=f'%(asctime)s,%(msecs)03d %(levelname)-8s [Rank {rank}] [%(filename)s:%(lineno)d] %(message)s',
        datefmt='%Y-%m-%d:%H:%M:%S'
    )

    # Create a console handler and set its formatter
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(formatter)

    # Add the handler to the logger
    if not logger.handlers:
        logger.addHandler(console_handler)

    # Optionally, create a file handler per rank
    """
    log_file = f'log_rank_{rank}.log'
    file_handler = logging.FileHandler(log_file)
    file_handler.setFormatter(formatter)
    logger.addHandler(file_handler)
    """

    # Load dataset and metadata
    metadata_path = os.path.join(args.data_dir, 'metadata.jsonl')
    metadata = load_metadata(metadata_path)
    df_generator = data_generator(args.data_dir)
    df_list = list(df_generator)  # Collect all (group_id, group_df) pairs

    dump_dir = Path(args.dump_dir) / "train"
    dump_dir.mkdir(parents=True, exist_ok=True)

    # Load tokenizer
    tokenizer = AutoTokenizer.from_pretrained(args.model_name, model_max_length=512)
    tokenizer.padding_side = "right"

    # Partition df_list among ranks
    df_list_per_rank = partition_list(df_list, world_size)
    my_df_list = df_list_per_rank[rank]

    # Load model instance onto device
    if args.use_bf16:
        logger.warning(f"Using bfloat16 for model {args.model_name}")
    model_instance = AutoModelForCausalLM.from_pretrained(
        args.model_name, torch_dtype=torch.bfloat16 if args.use_bf16 else None, device_map=device)
    model_instance.config.use_cache = False
    model_instance = model_instance.eval()

    state = load_state(dump_dir, rank)
    last_group_id = state.get("last_group_id", None) if state else None
    logger.warning(f"Rank {rank} last group_id processed: {last_group_id}")

    # Run training for assigned group_ids
    for group_id, group_df in my_df_list:
        if last_group_id is not None and group_id <= last_group_id:
            logger.warning(f"Rank {rank} skipping group_id {group_id} because it is already processed")
            continue
        logger.warning(f"Training models for group_id {group_id} on rank {rank}")
        for model_name in sorted(args.models.keys()):
            if model_name == "ReFT":
                logger.warning(f"Training {model_name} with group_id {group_id}")
                benchmark_model = getattr(axbench, model_name)(
                    model_instance, tokenizer, layer=args.layer,
                    training_args=args.models[model_name],
                    device=device, seed=args.seed
                )
                benchmark_model.make_model()
                if args.use_bf16:
                    benchmark_model.ax.to(torch.bfloat16)
                benchmark_model.train(group_df)
                benchmark_model.save(dump_dir, model_name=f"rank_{rank}_{model_name}")
                logger.warning(f"Saved weights and biases for model {model_name} on rank {rank}")
            else:
                for idx, concept in enumerate(metadata[group_id]["concepts"]):
                    logger.warning(f"Training {model_name} with concept {concept}")
                    benchmark_model = getattr(axbench, model_name)(
                        model_instance, tokenizer, layer=args.layer,
                        training_args=args.models[model_name],
                        device=device, seed=args.seed
                    )
                    benchmark_model.make_model()
                    if args.use_bf16:
                        benchmark_model.ax.to(torch.bfloat16)
                    binarized_df = binarize_df(group_df, concept, model_name)
                    benchmark_model.train(binarized_df)
                    benchmark_model.save(dump_dir, model_name=f"rank_{rank}_{model_name}")
                    logger.warning(f"Saved weights and biases for model {model_name} on rank {rank}")
            # Clean up
            del benchmark_model
            torch.cuda.empty_cache()
        # After processing, save state
        current_state = {'last_group_id': group_id}
        save_state(dump_dir, current_state, rank)

    # Synchronize all processes
    dist.barrier()

    # Rank 0 merges results
    if rank == 0:
        logger.warning("Rank 0 is merging results.")

        # Save other config
        config = {"model_name": args.model_name,
                "layer": args.layer,
                "component": args.component}
        config_path = dump_dir / CONFIG_FILE
        with open(config_path, 'w') as f:
            json.dump(config, f)

        for model_name in sorted(args.models.keys()):
            # Collect per-rank weight and bias files
            weight_files = [dump_dir / f"rank_{r}_{model_name}_weight.pt" for r in range(world_size)]
            bias_files = [dump_dir / f"rank_{r}_{model_name}_bias.pt" for r in range(world_size)]

            # Check if files exist
            weight_files_existing = [f for f in weight_files if f.exists()]
            bias_files_existing = [f for f in bias_files if f.exists()]

            if not weight_files_existing or not bias_files_existing:
                logger.warning(f"No weight or bias files found for model {model_name}. Skipping.")
                continue

            # Load weights and biases
            weights = [torch.load(f) for f in weight_files_existing]
            biases = [torch.load(f) for f in bias_files_existing]

            # Concatenate weights and biases
            merged_weight = torch.cat(weights, dim=0)
            merged_bias = torch.cat(biases, dim=0)

            # Save merged weight and bias files
            weight_file = dump_dir / f"{model_name}_weight.pt"
            bias_file = dump_dir / f"{model_name}_bias.pt"
            torch.save(merged_weight, weight_file)
            torch.save(merged_bias, bias_file)
            logger.warning(f"Saved merged weights and biases for model {model_name}")

            # Optionally delete per-rank files
            for f in weight_files_existing + bias_files_existing:
                try:
                    f.unlink()
                    logger.warning(f"Deleted file {f.name}")
                except Exception as e:
                    logger.error(f"Error deleting file {f.name}: {e}")

        # Save SAE weights and biases for inference
        logger.warning("Saving SAE weights and biases for inference")
        flatten_metadata = load_metadata_flatten(metadata_path)
        # Save pruned SAE weights and biases
        sae_path = flatten_metadata[0]["ref"].split("https://www.neuronpedia.org/")[-1]
        sae_url = f"https://www.neuronpedia.org/api/feature/{sae_path}"
        headers = {"X-Api-Key": os.environ.get("NP_API_KEY")}
        response = requests.get(sae_url, headers=headers).json()
        hf_repo = response["source"]["hfRepoId"]
        hf_folder = response["source"]["hfFolderId"]
        path_to_params = hf_hub_download(
            repo_id=hf_repo,
            filename=f"{hf_folder}/params.npz",
            force_download=False,
        )
        params = np.load(path_to_params)
        sae_pt_params = {k: torch.from_numpy(v) for k, v in params.items()}
        pruned_sae_pt_params = {
            "b_dec": sae_pt_params["b_dec"],
            "W_dec": [],
            "W_enc": [],
            "b_enc": [],
            "threshold": []
        }
        for concept_id, metadata in enumerate(flatten_metadata):
            sae_id = int(metadata["ref"].split("/")[-1])
            pruned_sae_pt_params["W_dec"].append(sae_pt_params["W_dec"][[sae_id], :])
            pruned_sae_pt_params["W_enc"].append(sae_pt_params["W_enc"][:, [sae_id]])
            pruned_sae_pt_params["b_enc"].append(sae_pt_params["b_enc"][[sae_id]])
            pruned_sae_pt_params["threshold"].append(sae_pt_params["threshold"][[sae_id]])
        for k, v in pruned_sae_pt_params.items():
            if k == "b_dec":
                continue
            if k == "W_enc":
                pruned_sae_pt_params[k] = torch.cat(v, dim=1)
            else:
                pruned_sae_pt_params[k] = torch.cat(v, dim=0)
        torch.save(pruned_sae_pt_params, dump_dir / "GemmaScopeSAE.pt") # sae only has one file

    # Finalize the process group
    dist.destroy_process_group()

    # Remove handlers to prevent duplication if the script is run multiple times
    logger.removeHandler(console_handler)
    # If file_handler is used, remove it as well
    # logger.removeHandler(file_handler)


if __name__ == "__main__":
    main()

