{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "848e3481-2812-4658-867a-63a82300e126",
   "metadata": {},
   "source": [
    "# Tutorial\n",
    "\n",
    "AxBench introduces two supervised dictionary-learning (SDL) methods that scale to thousands of concepts and outperform existing dictionary-learning approaches for LLMs. In this tutorial, we demonstrate one of these methods, ReFT-r1, which is built on the representation finetuning (ReFT) framework. ReFT-r1 provides a single dictionary of subspaces, with each subspace corresponding to a high-level concept. These subspaces can be used as a \"microscope\" to analyze model internals and to steer model behavior.\n",
    "\n",
    "**We will be using [pyvene](https://github.com/stanfordnlp/pyvene) to build interventions that load our SDLs.**\n",
    "\n",
    "**More about the ReFT-r1 with Concept16K** \n",
    "- It does not have an encoder-decoder structure. It is a big matrix where each row is a subspace.\n",
    "- The subspace serves two purposes: detection and steering.\n",
    "- The first version we release provides a dictionary of 16K subspaces.\n",
    "- These 16K concepts are adapted from Gemma model's SAEs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2721be56-6eab-44cf-85c9-fab56a98d79e",
   "metadata": {},
   "source": [
    "## Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed53a817-ab42-4090-93f0-bfec7992d478",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from huggingface_hub import hf_hub_download, notebook_login\n",
    "import numpy as np\n",
    "import torch, json, einops\n",
    "\n",
    "def load_jsonl(jsonl_path):\n",
    "    jsonl_data = []\n",
    "    with open(jsonl_path, 'r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            jsonl_data += [data]\n",
    "    return jsonl_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e91c529-ae7a-4ee0-b9a0-98fde6dc750b",
   "metadata": {},
   "source": [
    "In this tutorial, we will load `Gemma-2-2B-it` as well as our ReFT-r1 trained on the residual stream of layer 20. You will first need to log in to HugginFace so we can download related weights and data. Note that we are not using the pretrained model as ReFT-r1 is trained on the instruction-tuned one directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "388feba0-01bb-4ca1-be87-351ae175c410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2373f1df8f9646ee98fe988fefd1cd5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54be8ded-3141-47b6-8dc5-f2d15bea9f66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e12af9a3cb5241dd9b03487814471261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b-it\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ce8e71a-1218-46fb-96b6-b83457601b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer =  AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be475bb-67d6-4b8f-b59d-bfb2b51fd433",
   "metadata": {},
   "source": [
    "## Download our open ReFT-r1 SDL\n",
    "\n",
    "We provide the raw weights as well as the annotated concept metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34d2494b-0cae-4302-ad8f-f401735f62b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/qinanyu/ipykernel_2046021/2564112192.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  steering_vector = torch.load('../layer_10_addition/train/GemmaScopeSAE.pt')\n"
     ]
    }
   ],
   "source": [
    "steering_vector = torch.load('../layer_10_addition/train/GemmaScopeSAE.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8fd646-8e03-4625-a3c7-a20c8cf8d6ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8962adfc-46f9-444d-a9c4-84504d70f0ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241m.\u001b[39mtokenizer(\n\u001b[1;32m      2\u001b[0m                 input_strings, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      3\u001b[0m             )\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m      5\u001b[0m _, generations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39max_model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m      6\u001b[0m                 inputs, \n\u001b[1;32m      7\u001b[0m                 unit_locations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, intervene_on_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m                 temperature\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[1;32m     12\u001b[0m             )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "inputs = self.tokenizer(\n",
    "                input_strings, return_tensors=\"pt\", padding=True, truncation=True\n",
    "            ).to(self.device)\n",
    "\n",
    "_, generations = self.ax_model.generate(\n",
    "                inputs, \n",
    "                unit_locations=None, intervene_on_prompt=True, \n",
    "                subspaces=[{\"idx\": idx, \"mag\": mag, \"max_act\": max_acts, \n",
    "                            \"prefix_length\": kwargs[\"prefix_length\"]}]*self.num_of_layers,\n",
    "                max_new_tokens=eval_output_length, do_sample=True, \n",
    "                temperature=temperature,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89532376-2e19-4d09-ab88-044504357b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 2304])\n"
     ]
    }
   ],
   "source": [
    "print(steering_vector['W_dec'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "410b2221-7ccb-4279-96cc-8d1549350bb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'concept_id': 0,\n",
       " 'concept': 'the main thing this neuron does is respond to mathematical concepts focused on derivatives, activating with phrases that specify derivatives of mathematical functions, and then outputs a range of terms related to derivatives and their properties.',\n",
       " 'ref': 'https://www.neuronpedia.org/gemma-2-2b/10-gemmascope-res-65k/20527',\n",
       " 'concept_genres_map': {'the main thing this neuron does is respond to mathematical concepts focused on derivatives, activating with phrases that specify derivatives of mathematical functions, and then outputs a range of terms related to derivatives and their properties.': ['math']}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md = load_jsonl(\"../layer_10_addition/generate/metadata.jsonl\")\n",
    "md[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e90f8f2-bf1a-4b54-a501-5a264e086053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[     2,  18925,    692,    614,   3326,    577,   5056,   1593,   1069,\n",
      "           2177,    476,  47420,  18216, 235336]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "220878af9732492b9a2d6ce08d6d94ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Would you be able to travel through time using a wormhole?\n",
      "\n",
      "This question dives into scientific exploration, fantastical possibilities, and the limitations of our current understanding. \n",
      "\n",
      "**Let's analyze the scenario**:\n",
      "\n",
      "* **Wormholes:** These are theoretical tunnels through spacetime that could potentially connect two distant points in\n"
     ]
    }
   ],
   "source": [
    "# The input text\n",
    "prompt = \"Would you be able to travel through time using a wormhole?\"\n",
    "\n",
    "# Use the tokenizer to convert it to tokens. Note that this implicitly adds a special \"Beginning of Sequence\" or <bos> token to the start\n",
    "inputs = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=True).to(\"cuda\")\n",
    "print(inputs)\n",
    "\n",
    "# Pass it in to the model and generate text\n",
    "outputs = model.generate(inputs, max_new_tokens=50)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a0cf659-dd63-4fc2-b24e-666bafd48ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def steering(\n",
    "    activations, hook, steering_strength=1.0, steering_vector=None, max_act=1.0\n",
    "):\n",
    "    # Note if the feature fires anyway, we'd be adding to that here.\n",
    "    return activations + max_act * steering_strength * steering_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f412fa31-b99f-41c4-b8ed-474e3307f196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2304])\n"
     ]
    }
   ],
   "source": [
    "print(steering_vector['W_dec'][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5c98b34-87da-4f61-a452-05ecbffe0455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_steering(\n",
    "    model,\n",
    "    sae_sub,\n",
    "    prompt,\n",
    "    steering_feature,\n",
    "    max_act,\n",
    "    steering_strength=1.0,\n",
    "    max_new_tokens=95,\n",
    "):\n",
    "    input_ids = model.to_tokens(prompt, prepend_bos=True)\n",
    "\n",
    "    steering_vector = sae_sub['W_dec'][steering_feature].to('cuda')\n",
    "\n",
    "    steering_hook = partial(\n",
    "        steering,\n",
    "        steering_vector=steering_vector,\n",
    "        steering_strength=steering_strength,\n",
    "        max_act=max_act,\n",
    "    )\n",
    "\n",
    "    # standard transformerlens syntax for a hook context for generation\n",
    "    with model.hooks(fwd_hooks=[('blocks.10.hook_resid_post', steering_hook)]):\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            stop_at_eos=True,\n",
    "            prepend_bos=True,\n",
    "        )\n",
    "\n",
    "    return model.tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b70eb84a-486d-4fb8-971a-1f24bc44303b",
   "metadata": {},
   "outputs": [],
   "source": [
    "steering_feature = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ebe0cb8-a761-4447-9c0d-568640bc8538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a25474b33684fa0ac3d8b437af3d2d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/95 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Normal text (without steering):\n",
      "Would you be able to travel through time using a wormhole? \n",
      "\n",
      "Let's look at the science:\n",
      "\n",
      "1. **Wormholes: Theoretical** Our current understanding of physics doesn't allow us to create wormholes, these are proposed theoretical solutions that allow for connecting two distant points in spacetime. \n",
      "2. **Time Travel: Circular logic.** Some physicists suggest time travel could be theoretically possible, but time travel is still highly theoretical. \n",
      "3. **The Grandfather Paradox.** This famously problematic example is a logical inconsistency\n"
     ]
    }
   ],
   "source": [
    "normal_text = model.generate(\n",
    "    prompt,\n",
    "    max_new_tokens=95,\n",
    "    stop_at_eos=True,\n",
    "    prepend_bos=True,\n",
    ")\n",
    "\n",
    "print(\"\\nNormal text (without steering):\")\n",
    "print(normal_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f264de83-f7de-4fc6-a6cb-4bdad4a8a261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22a82e7e0beb4958b641372c829b5f03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/95 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steered text:\n",
      "<bos>Once upon a time, in a land far away, lived a little firefly named Flicker. Unlike other fireflies, Flicker's light wasn't a vibrant glow, but a dim, flickering flame. He felt shy and different, and he wished he could be like the other fireflies who shone bright and beautiful.\n",
      "\n",
      "One night, a wise old owl named Hoot saw Flicker struggling to light up. \"Why are you so sad, little one?\" Hoot asked\n"
     ]
    }
   ],
   "source": [
    "# Generate text with steering\n",
    "from functools import partial\n",
    "steered_text = generate_with_steering(\n",
    "    model, steering_vector, \"Once upon a time\", 0, 1.0, steering_strength=5.0\n",
    ")\n",
    "print(\"Steered text:\")\n",
    "print(steered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb0baf2-c33e-405c-b9be-836fabb6d431",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
